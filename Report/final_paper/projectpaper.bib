% example .bib file for use with projectpaper.tex


@article{rl_survey,
	author={Pack Kaelbling, Leslie and Littman, Michael L. and Moore, Andrew W.},
	title={Reinforcement Learning: A Survey},
	journal={Artifcial Intelligence Research},
	number="4",
	pages="237-285",
	year=1996
}

@article{sim_robot_arm,
	author={James, Stephen and Johns, Edward},
	title={3D Simulation for Robot Arm Control with Deep
Q-Learning},
	journal={NIPS 2016 Workshop: Deep Learning for Action and Interaction},
	year=2016
}

@article{nao_balance,
	author={Jin-Ling, Lin and Kao-Shing, Hwang},
	title={Balancing and Reconstruction of Segmented
Postures for Humanoid Robots in
Imitation of Motion},
	journal={IEEE Access},
	number="5",
	pages="17534-17542",
	year=2017
}

@article{arm_sim_door,
	author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
	year = {2017},
	month = {05},
	pages = {3389-3396},
	title = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}
}

@INPROCEEDINGS{robot_wheel, 
author={R. Hongge and W. Zhilong and L. Fujin and H. Meijie}, 
booktitle={The 26th Chinese Control and Decision Conference (2014 CCDC)}, 
title={The balance control of two-wheeled robot based on bionic learning algorithm}, 
year={2014}, 
volume={}, 
number={}, 
pages={4166-4170}, 
keywords={learning (artificial intelligence);mobile robots;motion control;neurocontrollers;GCS network;Q-learning;SOM network;bionic learning algorithm;growing cell structure;motion balance control;reinforcement learning;two-wheeled robot control;Biological neural networks;Learning (artificial intelligence);Mobile robots;Neurons;Vectors;Wheels;Balance control;Bionic learning;GCS network;Q-learning;Robot}, 
doi={10.1109/CCDC.2014.6852911}, 
ISSN={1948-9439}, 
month={May},
}

@INPROCEEDINGS{nn_then_dnn, 
author={Y. Yang and X. Li and L. Zhang}, 
booktitle={2016 12th World Congress on Intelligent Control and Automation (WCICA)}, 
title={Task-specific pre-learning to improve the convergence of reinforcement learning based on a deep neural network}, 
year={2016}, 
volume={}, 
number={}, 
pages={2209-2214}, 
keywords={approximation theory;convergence;learning (artificial intelligence);mobile robots;neural nets;autonomous movement controller;autonomous robots;convergence problem improvement;deep neural network;mountain-car problem;nonlinear neural network training;reinforcement learning convergence improvement;strong-reward function approximation;supervised learning;task-specific prelearning;two-phase reinforcement learning model;wheeled robot;Approximation algorithms;Automobiles;Convergence;Learning (artificial intelligence);Mathematical model;Neural networks;Robots}, 
doi={10.1109/WCICA.2016.7578787}, 
ISSN={}, 
month={June}
}

@INPROCEEDINGS{nao_football, 
author={M. A. Fahami and M. Roshanzamir and N. H. Izadi}, 
booktitle={2017 7th International Conference on Computer and Knowledge Engineering (ICCKE)}, 
title={A reinforcement learning approach to score goals in RoboCup 3D soccer simulation for nao humanoid robot}, 
year={2017}, 
volume={}, 
number={}, 
pages={450-454}, 
keywords={control engineering computing;digital simulation;humanoid robots;learning (artificial intelligence);mobile robots;multi-robot systems;sport;Nao robot;RoboCup 3D soccer simulation;RoboCup competitions;autonomous robots;hard coded instructions;learning process;nao humanoid robot;reinforcement learning;robot performance;score goals;Humanoid robots;Learning (artificial intelligence);Robot kinematics;Solid modeling;Three-dimensional displays;Training;Nao Humanoid Robot;Reinforcement Learning;RoboCup 3D Soccer simulation}, 
doi={10.1109/ICCKE.2017.8167920}, 
ISSN={}, 
month={Oct},}

@INPROCEEDINGS{air_hockey, 
author={A. Taitler and N. Shimkin}, 
booktitle={2017 International Conference on Control, Artificial Intelligence, Robotics Optimization (ICCAIRO)}, 
title={Learning Control for Air Hockey Striking Using Deep Reinforcement Learning}, 
year={2017}, 
volume={}, 
number={}, 
pages={22-27}, 
keywords={learning (artificial intelligence);learning systems;mobile robots;air hockey game;air hockey striking control striking;control policies;control signal;deep Q-learning algorithm feasible;direct command;model free deep reinforcement learning framework;robotic mechanism;standard learning scheme;Games;Heuristic algorithms;Machine learning;Neural networks;Robots;Standards}, 
doi={10.1109/ICCAIRO.2017.14}, 
ISSN={}, 
month={May},}

@article{er,
	author = {Adam, Sander and Busoniu, Lucian and BabuË‡ska, Robert},
	year = {2012},
	month = {03},
	pages = {201-212},
	title = {Experience Replay for Real-Time Reinforcement
Learning Control},
journal = {IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS},
volume={42},
number={2}
}

@article{er_deeper,
	author = {Zhang, Shangtong and Sutton, Richard S.},
	year = {2017},
	title = {A Deeper Look at Experience Replay},
journal = {31st Conference on Neural Information Processing Systems},
}
